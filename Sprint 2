Spliting of features and target

X = gold_data.drop(['Date','GLD'],axis=1)
Y = gold_data['GLD']


print(X)

print(Y)

Spliting of testing and training data

#training data of 80%
#test data of 20%
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=2)

Model Trainning

Random Forest Regression

regressor = RandomForestRegressor(n_estimators=100)

# training the model
regressor.fit(X_train,Y_train)

# prediction on Test Data
test_data_prediction = regressor.predict(X_test)

print(test_data_prediction)

from sklearn.metrics import mean_absolute_error
# Calculate the Mean Absolute Error (MAE)
R_mae = mean_absolute_error(Y_test, test_data_prediction)
# Print the MAE
print("Mean Absolute Error:", R_mae)

# R squared error
R_error_score = metrics.r2_score(Y_test, test_data_prediction)
print("R squared value : ", R_error_score)

Linear Regression


#Libraries for Linear regression
from sklearn.linear_model import LinearRegression

#fit the model
model = LinearRegression()
model.fit(X_train, Y_train)

# Make predictions on the test data
test_data_prediction2 = model.predict(X_test)

# Calculate Mean Absolute Error (MAE)
L_mae = mean_absolute_error(Y_test, test_data_prediction2)

# Print the results
print("Mean Absolute Error (MAE):", L_mae)


# R squared error
L_error_score = metrics.r2_score(Y_test, test_data_prediction2)
print("R squared value : ", L_error_score)

Gradient Boosting Regression


from sklearn.ensemble import GradientBoostingRegressor

regressor3 = GradientBoostingRegressor(n_estimators=100, random_state=2)

regressor3.fit(X_train, Y_train)

# Make predictions on the test data
test_data_prediction3 = regressor3.predict(X_test)

# Calculate Mean Absolute Error (MAE)
G_mae = mean_absolute_error(Y_test, test_data_prediction3)
# Print the results
print("Mean Absolute Error (MAE):", G_mae)

# R squared error
G_error_score = metrics.r2_score(Y_test, test_data_prediction3)
print("R squared value: ", G_error_score)

Comparative study


# Print the results
print("Mean Absolute Error (MAE):")
print("Linear Regression:", L_mae)
print("Random Forest Regression:", R_mae)
print("Gradient Boosting Regression:", G_mae)

print("\nR-squared (R2):")
print("Linear Regression:", L_error_score)
print("Random Forest Regression:", R_error_score)
print("Gradient Boosting Regression:", G_error_score)

#line plot of random forest regression
Y_test = list(Y_test)
plt.plot(Y_test, color='blue', label = 'Actual Value')
plt.plot(test_data_prediction, color='green', label='Predicted Value')
plt.title('Random Forest Actual Price vs Predicted Price')
plt.xlabel('Number of values')
plt.ylabel('GLD Price')
plt.legend()
plt.show()

# Prompt the user for input
print("Please enter feature values for evaluation:")
SPX = float(input("SPX value: "))
USO = float(input("USO value: "))
SLV = float(input("SLV value: "))
EUR_USD = float(input("EUR/USD value: "))

# Create a DataFrame with the user's input
user_data = {
    'SPX': [SPX],
    'USO': [USO],
    'SLV': [SLV],
    'EUR/USD': [EUR_USD]
}

user_df = pd.DataFrame(user_data)

# Make predictions using the model
predicted_labels = model.predict(user_df)

# Display the model's prediction
print("The model predicts a price of", predicted_labels[0])


SO concluding that random forest regression is best for the project


'''import pickle

# Save the model to a pickle file
with open('model.pkl', 'wb') as file:
    pickle.dump(model, file)'''

'''import joblib
model_filename="model1.pkl"
joblib.dump(regressor,model_filename)
from google.colab import files
files.download(model_filename)'''
